# TODO

* Fill out readme
* Allow for multiple models
  * Both versions of same model (e.g., llama-3.2-1b-instruct@q4_k_s and llama-3.2-1b-instruct@iq1_s) and different models (e.g., llama-3.2-1b-instruct@q4_k_s and mellum-4b-sft-python)
    * cf. default identifier in lmstudio
  * Default sort by file size
  * Analyze model size (stretch goal)
* Implement mlx if possible
  * https://github.com/Mozilla-Ocho/llamafile/discussions/779
  * https://github.com/cjpais/LocalScore
